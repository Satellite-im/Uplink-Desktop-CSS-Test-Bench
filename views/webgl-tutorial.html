<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
    <head>
        <link rel="stylesheet" href="/static/tools.css" type="text/css">
    </head>
<body style="display:flex; flex-direction:column; justify-content:space-between;">
    <span>video player</span>
    <canvas id="canvas" width="512" height="512">Browser doesn't support canvas. </canvas>
    <button onclick="play()">start</button>
    <script type="text/javascript" src="/webgl/webgl-utils.js"></script>
    <script type="text/javascript">

let first_click = false;
function play() {
    if (first_click === false) {
        first_click = true;
    } else {
        return;
    }
}

// glsl scripts and webgl2 adapted from this blog post: https://medium.com/docler-engineering/webgl-video-manipulation-8d0892b565b6
// chatgpt "helped" a little here too 

// stores the gl object
var glRef = {};
var programRef = {};
// the opengl texture, which is bound using gl.bindTexture() and then assigned to using the uniforms
var textures = {};
var attributes = {};
var buffers = {};
var uniforms = {};

const vertexShaderSource = `#version 300 es
in vec2 position;
out vec2 texCoord;
uniform vec2 u_resolution; 
  
void main() {  
    gl_Position =  vec4(( (position / u_resolution * 2.0) - 1.0)  * vec2(1, -1), 0, 1);
    texCoord = (position + 1.0) / 2.0;
}
`;

const fragmentShaderSource = `#version 300 es
precision mediump float;

uniform sampler2D u_image_frame;
in vec2 texCoord;
out vec4 fragColor;

void main() {
    // Sample Y, U, V values
    vec3 yuv = texture(u_image_frame, texCoord).rgb;

    // YUV to RGB conversion
    float Y = yuv.r;
    float U = yuv.g - 0.5;
    float V = yuv.b - 0.5;

    float R = clamp(Y + 1.13 * V, 0.0, 1.0) ;
    float G = clamp(Y - 0.39 * U - 0.58 * V, 0.0, 1.0) ;
    float B = clamp(Y + 2.03 * U, 0.0, 1.0) ;

    vec4 video_pixel = vec4( vec3(R,G,B), 1.0) ;
    fragColor = video_pixel ;
}
`;

function createShader(gl, type, source) {
    const shader = gl.createShader(type);
    gl.shaderSource(shader, source);
    gl.compileShader(shader);

    if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
        console.error(`Shader compilation error: ${gl.getShaderInfoLog(shader)}`);
        gl.deleteShader(shader);
        return null;
    }

    return shader;
}

function createProgram(gl, vertexShaderSource, fragmentShaderSource) {
    const vertexShader = createShader(gl, gl.VERTEX_SHADER, vertexShaderSource);
    const fragmentShader = createShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);

    if (!vertexShader || !fragmentShader) {
        return null;
    }

    const program = gl.createProgram();
    gl.attachShader(program, vertexShader);
    gl.attachShader(program, fragmentShader);
    gl.linkProgram(program);

    if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
        console.error(`Program linking error: ${gl.getProgramInfoLog(program)}`);
        gl.deleteProgram(program);
        return null;
    }

    return program;
}

function initTexture(gl) {
    // gl.bindTexture means 'apply all following texture functions to this texture'
    // TEXTURE_MIN_FILTER determines how texture data is sampled when the texture is smaller than the source image
    // TEXTURE_MAG_FILTER ''                                                         larger than the source image

    const texture = gl.createTexture();
    gl.bindTexture(gl.TEXTURE_2D, texture);
    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);
    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);

    textures.zero = texture;
}

function init() {
    const canvas = document.querySelector("#canvas") || document.createElement("canvas");
    canvas.width = 512;
    canvas.height = 512;

    const gl = canvas.getContext("webgl2");
    if (!gl) {
        console.error("Unable to initialize WebGL. Your browser may not support it.");
        return;
    }
   
    const program = createProgram(gl, vertexShaderSource, fragmentShaderSource);
    gl.useProgram(program);

    // init attributes
    const positionLocation = gl.getAttribLocation(program, "position");
    attributes.positionLocation = positionLocation;

    // init buffers
    const positionBuffer = gl.createBuffer();
    buffers.positionBuffer = positionBuffer;
    // todo: this is probably wrong
    const positions = new Float32Array([
      0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0,
    ]);
    gl.bindBuffer(gl.ARRAY_BUFFER, buffers.positionBuffer);
    gl.bufferData(gl.ARRAY_BUFFER, positions, gl.STATIC_DRAW);
    gl.vertexAttribPointer(attributes.positionLocation, 2, gl.FLOAT, false, 0, 0);
    gl.enableVertexAttribArray(attributes.positionLocation);

    initTexture(gl);

    const u_image_frame = gl.getUniformLocation(program, "u_image_frame");
    gl.uniform1i(u_image_frame, 0); // texture unit 0
    uniforms.image_frame = u_image_frame;

    // set which texture units to render with.
    gl.activeTexture(gl.TEXTURE0);
    gl.bindTexture(gl.TEXTURE_2D, textures.zero);

    gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);
    gl.clearColor(0, 0, 0, 0);
    gl.clear(gl.COLOR_BUFFER_BIT);

    // Bind the position buffer.
    gl.bindBuffer(gl.ARRAY_BUFFER, buffers.positionBuffer);

    // Tell the position attribute how to get data out of positionBuffer (ARRAY_BUFFER)
    {
        const size = 2;
        const type = gl.FLOAT;
        const normalize = false;
        const stride = 0;
        const offset = 0;
        gl.vertexAttribPointer(
            positionLocation,
            size,
            type,
            normalize,
            stride,
            offset
        );
    }

    // lookup uniforms
    const resolutionLocation = gl.getUniformLocation(program, "u_resolution");
    // set the resolution
    gl.uniform2f(resolutionLocation, gl.canvas.width, gl.canvas.height);

    // Draw the rectangle.
    gl.drawArrays(gl.TRIANGLES, 0, 6);

    programRef.program = program;
    glRef.gl = gl;
}

function render(data, width, height) {
    // Assuming you have the Y, U, and V components in separate arrays 
    glRef.gl.bindTexture(glRef.gl.TEXTURE_2D, textures.zero);
    glRef.gl.texImage2D(
        glRef.gl.TEXTURE_2D, 
        0, 
        glRef.gl.LUMINANCE, 
        width, 
        height * 1.5, 
        0, 
        glRef.gl.LUMINANCE, 
        glRef.gl.UNSIGNED_BYTE, 
        data
    );
   
    glRef.gl.clear(glRef.gl.COLOR_BUFFER_BIT);
    glRef.gl.drawArrays(glRef.gl.TRIANGLES, 0, 6);
}

init();

const xhr=new XMLHttpRequest();
xhr.open('GET','/static/letter-f.yuv');
xhr.responseType='arraybuffer';
xhr.onload=()=>{
    const data=xhr.response;
    render(data, 475, 475);
}
xhr.send();

// todo: receive images and call render()
// left off here: https://chat.openai.com/c/f4829fe9-0ff2-42a6-bb40-9b0fd98ad6c4

    </script>
</body>
</html>